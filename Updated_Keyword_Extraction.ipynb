{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of recent feedback, this section is meant to filter out certain words that the STOP_WORDS function in spacy does not handle. Feel free to add any words to this list that you would like. This model was built on the idea of using the lemma of words to count frequency. The lemma of a word is essentially the base of the word. \n",
    "\n",
    "Currently, the code is set to lemmatize the text, so infinite verb forms are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Common French nouns and modified verbs to exclude from keywords\n",
    "# You can add or delete anything between the {} , making sure to enclose it in quotes \"\"\n",
    "COMMON_FRENCH_WORDS = {\n",
    "    \"euh\", \"alors\", \"ben\", \"bon\", \"bah\", \"voilà\", \"tu vois\", \"enfin\", \"quoi\",\n",
    "    \"c'est-à-dire\", \"hein\", \"genre\", \"tu sais\", \"bah oui\", \"eh bien\", \"d'accord\",\n",
    "    \"ah bon\", \"écoute\", \"tu comprends\", \"en fait\", \"jour\", \"nuit\", \"heure\",\n",
    "    \"temps\", \"chose\", \"année\", \"moment\", \"question\", \"réponse\", \"vraiment\", \"vois\",\n",
    "    \"lieu\", \"été\", \"faire\", \"bien\", \"euh\", \"non\", \"oui\", \"aller\", \"passer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_keywords(file_path, num_keywords=50):\n",
    "    # Load French language model\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # Read file content\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization and filtering\n",
    "    lemmatized_words = [\n",
    "        token.lemma_.lower() for token in doc \n",
    "        if token.is_alpha and token.text.lower() not in STOP_WORDS and token.text.lower() not in COMMON_FRENCH_WORDS\n",
    "    ]\n",
    "    \n",
    "    # Calculate word frequencies using the Counter function\n",
    "    word_freq = Counter(lemmatized_words)\n",
    "    \n",
    "    # Get the top 'N' keywords\n",
    "    keywords = word_freq.most_common(num_keywords)\n",
    "    \n",
    "    return dict(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_word_cloud(keywords):\n",
    "    # Change the colors of the word cloud however you like\n",
    "    # Examples include: red, orange, skyblue, black, lightgreen, etc.\n",
    "    # You can reference the documentation for word cloud for all possible colors.\n",
    "    colors = LinearSegmentedColormap.from_list(\n",
    "        \"Colors\", [\"green\", \"beige\", \"darkgreen\"], N=256\n",
    "    )\n",
    "    \n",
    "    # This part generates the word cloud using the extracted keywords.\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='orange',\n",
    "        colormap=colors\n",
    "    ).generate_from_frequencies(keywords)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud of Extracted Keywords\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_bar_chart(keywords):\n",
    "    # Extract words and their frequencies\n",
    "    words = list(keywords.keys())\n",
    "    freqs = list(keywords.values())\n",
    "    \n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, freqs, color='blue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Keywords')\n",
    "    plt.title('Top Keywords Extracted')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "    plt.show()\n",
    "\n",
    "# This part is for the file importation\n",
    "if __name__ == \"__main__\":\n",
    "    # Change this file name to any \n",
    "    file_name = \"interview_responses_new.txt\"\n",
    "    keyword_frequencies = extract_keywords(file_name, num_keywords=15)\n",
    "    if keyword_frequencies:\n",
    "        print(\"Top Keywords:\", list(keyword_frequencies.keys()))\n",
    "        \n",
    "        # Visualize the keywords with a word cloud\n",
    "        visualize_word_cloud(keyword_frequencies)\n",
    "        \n",
    "        # Visualize the keywords with a bar chart\n",
    "        visualize_bar_chart(keyword_frequencies)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
